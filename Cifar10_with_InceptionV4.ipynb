{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cifar10_with_InceptionV4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BraneXZ/CNN_Cifar10_with_InceptionV4/blob/master/Cifar10_with_InceptionV4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Nk-kvagfr5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tensorflow 2.0 beta and tensorflow datasets\n",
        "!pip install tensorflow-gpu==2.0.0-beta1\n",
        "!pip install tensorflow-datasets\n",
        "\n",
        "# tenorflow\n",
        "import tensorflow as     tf\n",
        "from   tensorflow import keras\n",
        "\n",
        "# tensorflow datasets\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# additional libraries\n",
        "import math\n",
        "import numpy             as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLvLlA8HNpiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tPVVqd2f2fF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data\n",
        "DATA_NUM_CLASSES        = 10\n",
        "DATA_CHANNELS           = 3\n",
        "DATA_ROWS               = 32\n",
        "DATA_COLS               = 32\n",
        "DATA_CROP_ROWS          = 28\n",
        "DATA_CROP_COLS          = 28\n",
        "DATA_MEAN               = np.array([[[125.30691805, 122.95039414, 113.86538318]]]) # CIFAR10\n",
        "DATA_STD_DEV            = np.array([[[ 62.99321928,  62.08870764,  66.70489964]]]) # CIFAR10\n",
        "\n",
        "# model\n",
        "INCEPTION_A    = 3\n",
        "INCEPTION_B    = 7\n",
        "INCEPTION_C    = 4\n",
        "\n",
        "# training\n",
        "TRAINING_BATCH_SIZE      = 32\n",
        "TRAINING_SHUFFLE_BUFFER  = 5000\n",
        "TRAINING_BN_MOMENTUM     = 0.99\n",
        "TRAINING_BN_EPSILON      = 0.001\n",
        "TRAINING_LR_MAX          = 0.001\n",
        "# TRAINING_LR_SCALE        = 0.1\n",
        "# TRAINING_LR_EPOCHS       = 2\n",
        "TRAINING_LR_INIT_SCALE   = 0.01\n",
        "TRAINING_LR_INIT_EPOCHS  = 5\n",
        "TRAINING_LR_FINAL_SCALE  = 0.01\n",
        "TRAINING_LR_FINAL_EPOCHS = 55\n",
        "\n",
        "# training (derived)\n",
        "TRAINING_NUM_EPOCHS = TRAINING_LR_INIT_EPOCHS + TRAINING_LR_FINAL_EPOCHS\n",
        "TRAINING_LR_INIT    = TRAINING_LR_MAX*TRAINING_LR_INIT_SCALE\n",
        "TRAINING_LR_FINAL   = TRAINING_LR_MAX*TRAINING_LR_FINAL_SCALE\n",
        "\n",
        "# saving\n",
        "SAVE_MODEL_PATH = '/content/drive/My Drive/save/model_long/'\n",
        "!mkdir -p \"$SAVE_MODEL_PATH\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwkMQ5XRf_n3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pre processing for training data\n",
        "def pre_processing_train(example):\n",
        "\n",
        "    # extract image and label from example\n",
        "    image = example[\"image\"]\n",
        "    label = example[\"label\"]\n",
        "  \n",
        "    # image is cast to float32, normalized, augmented and random cropped\n",
        "    # label is cast to int32\n",
        "    image = tf.math.divide(tf.math.subtract(tf.dtypes.cast(image, tf.float32), DATA_MEAN), DATA_STD_DEV)\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_crop(image, size=[DATA_CROP_ROWS, DATA_CROP_COLS, 3])\n",
        "    label = tf.dtypes.cast(label, tf.int32)\n",
        "    \n",
        "    # return image and label\n",
        "    return image, label\n",
        "\n",
        "# pre processing for testing data\n",
        "def pre_processing_test(example):\n",
        "\n",
        "    # extract image and label from example\n",
        "    image = example[\"image\"]\n",
        "    label = example[\"label\"]\n",
        "\n",
        "    # image is cast to float32, normalized, augmented and center cropped\n",
        "    # label is cast to int32\n",
        "    image = tf.math.divide(tf.math.subtract(tf.dtypes.cast(image, tf.float32), DATA_MEAN), DATA_STD_DEV)\n",
        "    image = tf.image.crop_to_bounding_box(image, (DATA_ROWS - DATA_CROP_ROWS) // 2, (DATA_COLS - DATA_CROP_COLS) // 2, DATA_CROP_ROWS, DATA_CROP_COLS)\n",
        "    label = tf.dtypes.cast(label, tf.int32)\n",
        "    \n",
        "    # return image and label\n",
        "    return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s_CMBwmgDSr",
        "colab_type": "code",
        "outputId": "4ae94aa2-3922-479d-ea2f-0fec4ca3b57f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# download data and split into training and testing datasets\n",
        "dataset_train, info = tfds.load(\"cifar10\", split=tfds.Split.TRAIN, with_info=True)\n",
        "dataset_test,  info = tfds.load(\"cifar10\", split=tfds.Split.TEST,  with_info=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTSeDlU3gFeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# transform training dataset\n",
        "dataset_train = dataset_train.map(pre_processing_train, num_parallel_calls=4)\n",
        "dataset_train = dataset_train.shuffle(buffer_size=TRAINING_SHUFFLE_BUFFER)\n",
        "dataset_train = dataset_train.batch(TRAINING_BATCH_SIZE)\n",
        "dataset_train = dataset_train.prefetch(buffer_size=1)\n",
        "\n",
        "# transform testing dataset\n",
        "dataset_test = dataset_test.map(pre_processing_test, num_parallel_calls=4)\n",
        "dataset_test = dataset_test.batch(TRAINING_BATCH_SIZE)\n",
        "dataset_test = dataset_test.prefetch(buffer_size=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYYj6zs4gIba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(rows, cols, channels, num_classes, lr_initial):\n",
        "  # encoder - input\n",
        "  model_input = keras.Input(shape=(rows, cols, channels), name='input_image')\n",
        "  x           = model_input\n",
        "\n",
        "  # encoder - tail\n",
        "  x = keras.layers.Conv2D(4, 3, strides=1, padding=\"same\", activation=None, use_bias=False)(x)\n",
        "\n",
        "  # Inception A\n",
        "  for i in range (INCEPTION_A):\n",
        "    residual1 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "    residual1 = keras.layers.ReLU()(residual1)\n",
        "    residual1 = keras.layers.AveragePooling2D(strides=1, padding=\"same\")(residual1)\n",
        "    residual1 = keras.layers.Conv2D(12, 1, strides=1, padding=\"same\", activation=None, use_bias=False)(residual1)\n",
        "    \n",
        "    residual2 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "    residual2 = keras.layers.ReLU()(residual2)\n",
        "    residual2 = keras.layers.Conv2D(12, 1, strides=1, padding=\"same\", activation=None, use_bias=False)(residual2)\n",
        "    \n",
        "    residual3 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "    residual3 = keras.layers.ReLU()(residual3)\n",
        "    residual3 = keras.layers.Conv2D(8, 1, strides=1, padding=\"same\", activation=None, use_bias=False)(residual3)\n",
        "    residual3 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual3)\n",
        "    residual3 = keras.layers.ReLU()(residual3)\n",
        "    residual3 = keras.layers.Conv2D(12, 3, strides=1, padding=\"same\", activation=None, use_bias=False)(residual3)\n",
        "    \n",
        "    residual4 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "    residual4 = keras.layers.ReLU()(residual4)\n",
        "    residual4 = keras.layers.Conv2D(8, 1, strides=1, padding=\"same\", activation=None, use_bias=False)(residual4)\n",
        "    residual4 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual4)\n",
        "    residual4 = keras.layers.ReLU()(residual4)\n",
        "    residual4 = keras.layers.Conv2D(12, 3, strides=1, padding=\"same\", activation=None, use_bias=False)(residual4)\n",
        "    residual4 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual4)\n",
        "    residual4 = keras.layers.ReLU()(residual4)\n",
        "    residual4 = keras.layers.Conv2D(12, 3, strides=1, padding=\"same\", activation=None, use_bias=False)(residual4)\n",
        "    \n",
        "    x = keras.layers.Concatenate()([residual1, residual2, residual3, residual4])\n",
        "  \n",
        "  # Reduction A\n",
        "  residual1 = keras.layers.MaxPooling2D(3, strides=2, padding=\"valid\")(x)\n",
        "  \n",
        "  residual2 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "  residual2 = keras.layers.ReLU()(residual2)\n",
        "  residual2 = keras.layers.Conv2D(48, 3, strides=2, padding=\"valid\", activation=None, use_bias=False)(residual2)\n",
        "  \n",
        "  residual3 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "  residual3 = keras.layers.ReLU()(residual3)\n",
        "  residual3 = keras.layers.Conv2D(24, 1, strides=1, padding=\"same\", activation=None, use_bias=False)(residual3)\n",
        "  residual3 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "  residual3 = keras.layers.ReLU()(residual3)\n",
        "  residual3 = keras.layers.Conv2D(28, 3, strides=1, padding=\"same\", activation=None, use_bias=False)(residual3)\n",
        "  residual3 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "  residual3 = keras.layers.ReLU()(residual3)\n",
        "  residual3 = keras.layers.Conv2D(32, 3, strides=2, padding=\"valid\", activation=None, use_bias=False)(residual3)\n",
        "  \n",
        "  x = keras.layers.Concatenate()([residual1, residual2, residual3])\n",
        "\n",
        "  # Inception B\n",
        "  for i in range (INCEPTION_B):\n",
        "    residual1 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "    residual1 = keras.layers.ReLU()(residual1)\n",
        "    residual1 = keras.layers.AveragePooling2D(strides=1, padding=\"same\")(residual1)\n",
        "    residual1 = keras.layers.Conv2D(16, 1, strides=1, padding=\"same\", activation=None, use_bias=False)(residual1)\n",
        "    \n",
        "    residual2 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "    residual2 = keras.layers.ReLU()(residual2)\n",
        "    residual2 = keras.layers.Conv2D(48, 1, strides=1, padding=\"same\", activation=None, use_bias=False)(residual2)\n",
        "    \n",
        "    residual3 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "    residual3 = keras.layers.ReLU()(residual3)\n",
        "    residual3 = keras.layers.Conv2D(24, 1, strides=1, padding=\"same\", activation=None, use_bias=False)(residual3)\n",
        "    residual3 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual3)\n",
        "    residual3 = keras.layers.ReLU()(residual3)\n",
        "    residual3 = keras.layers.Conv2D(28, (1, 7), strides=1, padding=\"same\", activation=None, use_bias=False)(residual3)\n",
        "    residual3 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual3)\n",
        "    residual3 = keras.layers.ReLU()(residual3)\n",
        "    residual3 = keras.layers.Conv2D(32, (1, 7), strides=1, padding=\"same\", activation=None, use_bias=False)(residual3)\n",
        "    \n",
        "    residual4 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "    residual4 = keras.layers.ReLU()(residual4)\n",
        "    residual4 = keras.layers.Conv2D(24, 1, strides=1, padding=\"same\", activation=None, use_bias=False)(residual4)\n",
        "    residual4 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual4)\n",
        "    residual4 = keras.layers.ReLU()(residual4)\n",
        "    residual4 = keras.layers.Conv2D(24, (1, 7), strides=1, padding=\"same\", activation=None, use_bias=False)(residual4)\n",
        "    residual4 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual4)\n",
        "    residual4 = keras.layers.ReLU()(residual4)\n",
        "    residual4 = keras.layers.Conv2D(28, (7, 1), strides=1, padding=\"same\", activation=None, use_bias=False)(residual4)\n",
        "    residual4 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual4)\n",
        "    residual4 = keras.layers.ReLU()(residual4)\n",
        "    residual4 = keras.layers.Conv2D(28, (1, 7), strides=1, padding=\"same\", activation=None, use_bias=False)(residual4)\n",
        "    residual4 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual4)\n",
        "    residual4 = keras.layers.ReLU()(residual4)\n",
        "    residual4 = keras.layers.Conv2D(32, (7, 1), strides=1, padding=\"same\", activation=None, use_bias=False)(residual4)\n",
        "    \n",
        "    x = keras.layers.Concatenate()([residual1, residual2, residual3, residual4])\n",
        "    \n",
        "  # Reduction B\n",
        "  residual1 = keras.layers.MaxPooling2D(3, strides=2, padding=\"valid\")(x)\n",
        "  \n",
        "  residual2 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "  residual2 = keras.layers.ReLU()(residual2)\n",
        "  residual2 = keras.layers.Conv2D(24, 1, strides=1, padding=\"same\", activation=None, use_bias=False)(residual2)\n",
        "  residual2 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual2)\n",
        "  residual2 = keras.layers.ReLU()(residual2)\n",
        "  residual2 = keras.layers.Conv2D(24, 3, strides=2, padding=\"valid\", activation=None, use_bias=False)(residual2)\n",
        "  \n",
        "  residual3 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "  residual3 = keras.layers.ReLU()(residual3)\n",
        "  residual3 = keras.layers.Conv2D(32, 1, strides=1, padding=\"same\", activation=None, use_bias=False)(residual3)\n",
        "  residual3 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "  residual3 = keras.layers.ReLU()(residual3)\n",
        "  residual3 = keras.layers.Conv2D(32, (1, 7), strides=1, padding=\"same\", activation=None, use_bias=False)(residual3)\n",
        "  residual3 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "  residual3 = keras.layers.ReLU()(residual3)\n",
        "  residual3 = keras.layers.Conv2D(40, (7, 1), strides=1, padding=\"same\", activation=None, use_bias=False)(residual3)\n",
        "  residual3 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "  residual3 = keras.layers.ReLU()(residual3)\n",
        "  residual3 = keras.layers.Conv2D(40, 3, strides=2, padding=\"valid\", activation=None, use_bias=False)(residual3)\n",
        "  \n",
        "  x = keras.layers.Concatenate()([residual1, residual2, residual3])\n",
        "  \n",
        "\n",
        "  # Inception C\n",
        "  for i in range (INCEPTION_C):\n",
        "    residual1 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "    residual1 = keras.layers.ReLU()(residual1)\n",
        "    residual1 = keras.layers.AveragePooling2D(strides=1, padding=\"same\")(residual1)\n",
        "    residual1 = keras.layers.Conv2D(32, 1, strides=1, padding=\"same\", activation=None, use_bias=False)(residual1)\n",
        "    \n",
        "    residual2 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "    residual2 = keras.layers.ReLU()(residual2)\n",
        "    residual2 = keras.layers.Conv2D(32, 1, strides=1, padding=\"same\", activation=None, use_bias=False)(residual2)\n",
        "    \n",
        "    residual3 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "    residual3 = keras.layers.ReLU()(residual3)\n",
        "    residual3 = keras.layers.Conv2D(48, 1, strides=1, padding=\"same\", activation=None, use_bias=False)(residual3)\n",
        "    residual3 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual3)\n",
        "    residual3 = keras.layers.ReLU()(residual3)\n",
        "    residual3_1 = keras.layers.Conv2D(32, (1, 3), strides=1, padding=\"same\", activation=None, use_bias=False)(residual3)\n",
        "    residual3_2 = keras.layers.Conv2D(32, (3, 1), strides=1, padding=\"same\", activation=None, use_bias=False)(residual3)\n",
        "    \n",
        "    residual4 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "    residual4 = keras.layers.ReLU()(residual4)\n",
        "    residual4 = keras.layers.Conv2D(48, 1, strides=1, padding=\"same\", activation=None, use_bias=False)(residual4)\n",
        "    residual4 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual4)\n",
        "    residual4 = keras.layers.ReLU()(residual4)\n",
        "    residual4 = keras.layers.Conv2D(56, (1, 3), strides=1, padding=\"same\", activation=None, use_bias=False)(residual4)\n",
        "    residual4 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual4)\n",
        "    residual4 = keras.layers.ReLU()(residual4)\n",
        "    residual4 = keras.layers.Conv2D(64, (3, 1), strides=1, padding=\"same\", activation=None, use_bias=False)(residual4)\n",
        "    residual4 = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual4)\n",
        "    residual4 = keras.layers.ReLU()(residual4)\n",
        "    residual4_1 = keras.layers.Conv2D(32, (1, 3), strides=1, padding=\"same\", activation=None, use_bias=False)(residual4)\n",
        "    residual4_2 = keras.layers.Conv2D(32, (3, 1), strides=1, padding=\"same\", activation=None, use_bias=False)(residual4)\n",
        "    \n",
        "    x = keras.layers.Concatenate()([residual1, residual2, residual3_1, residual3_2, residual4_1, residual4_2])\n",
        "  \n",
        "  x = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "  x = keras.layers.ReLU()(x)\n",
        "  \n",
        "  #encoder - output\n",
        "  encoder_output = x\n",
        "  \n",
        "  y = keras.layers.GlobalAveragePooling2D()(encoder_output)\n",
        "  y = keras.layers.Dropout(0.2)(y)\n",
        "  decoder_output = keras.layers.Dense(num_classes, activation='softmax')(y)\n",
        "\n",
        "  # forward path\n",
        "  model = keras.Model(inputs=model_input, outputs=decoder_output, name='inceptionV4_model')\n",
        "\n",
        "  # loss, backward path (implicit) and weight update\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr_initial), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  # return model\n",
        "  return model\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBVvjjG8isUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_model(28, 28, 3, DATA_NUM_CLASSES, TRAINING_LR_MAX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGZ-dE-TizGv",
        "colab_type": "code",
        "outputId": "98a21901-00d4-48dc-e81a-93c5214514ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"inceptionV4_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_image (InputLayer)        [(None, 28, 28, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_234 (Conv2D)             (None, 28, 28, 4)    108         input_image[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_228 (BatchN (None, 28, 28, 4)    16          conv2d_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_228 (ReLU)                (None, 28, 28, 4)    0           batch_normalization_228[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_239 (Conv2D)             (None, 28, 28, 8)    32          re_lu_228[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_226 (BatchN (None, 28, 28, 4)    16          conv2d_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_229 (BatchN (None, 28, 28, 8)    32          conv2d_239[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_226 (ReLU)                (None, 28, 28, 4)    0           batch_normalization_226[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_229 (ReLU)                (None, 28, 28, 8)    0           batch_normalization_229[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_224 (BatchN (None, 28, 28, 4)    16          conv2d_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_237 (Conv2D)             (None, 28, 28, 8)    32          re_lu_226[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_240 (Conv2D)             (None, 28, 28, 12)   864         re_lu_229[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_224 (ReLU)                (None, 28, 28, 4)    0           batch_normalization_224[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_225 (BatchN (None, 28, 28, 4)    16          conv2d_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_227 (BatchN (None, 28, 28, 8)    32          conv2d_237[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_230 (BatchN (None, 28, 28, 12)   48          conv2d_240[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_23 (AveragePo (None, 28, 28, 4)    0           re_lu_224[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_225 (ReLU)                (None, 28, 28, 4)    0           batch_normalization_225[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_227 (ReLU)                (None, 28, 28, 8)    0           batch_normalization_227[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_230 (ReLU)                (None, 28, 28, 12)   0           batch_normalization_230[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_235 (Conv2D)             (None, 28, 28, 12)   48          average_pooling2d_23[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_236 (Conv2D)             (None, 28, 28, 12)   48          re_lu_225[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_238 (Conv2D)             (None, 28, 28, 12)   864         re_lu_227[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_241 (Conv2D)             (None, 28, 28, 12)   1296        re_lu_230[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 28, 28, 48)   0           conv2d_235[0][0]                 \n",
            "                                                                 conv2d_236[0][0]                 \n",
            "                                                                 conv2d_238[0][0]                 \n",
            "                                                                 conv2d_241[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_231 (BatchN (None, 28, 28, 48)   192         concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_234 (BatchN (None, 28, 28, 48)   192         concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_231 (ReLU)                (None, 28, 28, 48)   0           batch_normalization_231[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_234 (ReLU)                (None, 28, 28, 48)   0           batch_normalization_234[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 13, 13, 48)   0           concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_242 (Conv2D)             (None, 13, 13, 48)   20736       re_lu_231[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_245 (Conv2D)             (None, 13, 13, 32)   13824       re_lu_234[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 13, 13, 128)  0           max_pooling2d_4[0][0]            \n",
            "                                                                 conv2d_242[0][0]                 \n",
            "                                                                 conv2d_245[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_240 (BatchN (None, 13, 13, 128)  512         concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_240 (ReLU)                (None, 13, 13, 128)  0           batch_normalization_240[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_251 (Conv2D)             (None, 13, 13, 24)   3072        re_lu_240[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_241 (BatchN (None, 13, 13, 24)   96          conv2d_251[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_241 (ReLU)                (None, 13, 13, 24)   0           batch_normalization_241[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_252 (Conv2D)             (None, 13, 13, 24)   4032        re_lu_241[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_237 (BatchN (None, 13, 13, 128)  512         concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_242 (BatchN (None, 13, 13, 24)   96          conv2d_252[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_237 (ReLU)                (None, 13, 13, 128)  0           batch_normalization_237[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_242 (ReLU)                (None, 13, 13, 24)   0           batch_normalization_242[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_248 (Conv2D)             (None, 13, 13, 24)   3072        re_lu_237[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_253 (Conv2D)             (None, 13, 13, 28)   4704        re_lu_242[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_238 (BatchN (None, 13, 13, 24)   96          conv2d_248[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_243 (BatchN (None, 13, 13, 28)   112         conv2d_253[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_238 (ReLU)                (None, 13, 13, 24)   0           batch_normalization_238[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_243 (ReLU)                (None, 13, 13, 28)   0           batch_normalization_243[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_235 (BatchN (None, 13, 13, 128)  512         concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_249 (Conv2D)             (None, 13, 13, 28)   4704        re_lu_238[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_254 (Conv2D)             (None, 13, 13, 28)   5488        re_lu_243[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_235 (ReLU)                (None, 13, 13, 128)  0           batch_normalization_235[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_236 (BatchN (None, 13, 13, 128)  512         concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_239 (BatchN (None, 13, 13, 28)   112         conv2d_249[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_244 (BatchN (None, 13, 13, 28)   112         conv2d_254[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_24 (AveragePo (None, 13, 13, 128)  0           re_lu_235[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_236 (ReLU)                (None, 13, 13, 128)  0           batch_normalization_236[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_239 (ReLU)                (None, 13, 13, 28)   0           batch_normalization_239[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_244 (ReLU)                (None, 13, 13, 28)   0           batch_normalization_244[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_246 (Conv2D)             (None, 13, 13, 16)   2048        average_pooling2d_24[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_247 (Conv2D)             (None, 13, 13, 48)   6144        re_lu_236[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_250 (Conv2D)             (None, 13, 13, 32)   6272        re_lu_239[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_255 (Conv2D)             (None, 13, 13, 32)   6272        re_lu_244[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 13, 13, 128)  0           conv2d_246[0][0]                 \n",
            "                                                                 conv2d_247[0][0]                 \n",
            "                                                                 conv2d_250[0][0]                 \n",
            "                                                                 conv2d_255[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_250 (BatchN (None, 13, 13, 128)  512         concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_250 (ReLU)                (None, 13, 13, 128)  0           batch_normalization_250[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_261 (Conv2D)             (None, 13, 13, 24)   3072        re_lu_250[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_251 (BatchN (None, 13, 13, 24)   96          conv2d_261[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_251 (ReLU)                (None, 13, 13, 24)   0           batch_normalization_251[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_262 (Conv2D)             (None, 13, 13, 24)   4032        re_lu_251[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_247 (BatchN (None, 13, 13, 128)  512         concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_252 (BatchN (None, 13, 13, 24)   96          conv2d_262[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_247 (ReLU)                (None, 13, 13, 128)  0           batch_normalization_247[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_252 (ReLU)                (None, 13, 13, 24)   0           batch_normalization_252[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_258 (Conv2D)             (None, 13, 13, 24)   3072        re_lu_247[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_263 (Conv2D)             (None, 13, 13, 28)   4704        re_lu_252[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_248 (BatchN (None, 13, 13, 24)   96          conv2d_258[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_253 (BatchN (None, 13, 13, 28)   112         conv2d_263[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_248 (ReLU)                (None, 13, 13, 24)   0           batch_normalization_248[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_253 (ReLU)                (None, 13, 13, 28)   0           batch_normalization_253[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_245 (BatchN (None, 13, 13, 128)  512         concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_259 (Conv2D)             (None, 13, 13, 28)   4704        re_lu_248[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_264 (Conv2D)             (None, 13, 13, 28)   5488        re_lu_253[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_245 (ReLU)                (None, 13, 13, 128)  0           batch_normalization_245[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_246 (BatchN (None, 13, 13, 128)  512         concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_249 (BatchN (None, 13, 13, 28)   112         conv2d_259[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_254 (BatchN (None, 13, 13, 28)   112         conv2d_264[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_25 (AveragePo (None, 13, 13, 128)  0           re_lu_245[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_246 (ReLU)                (None, 13, 13, 128)  0           batch_normalization_246[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_249 (ReLU)                (None, 13, 13, 28)   0           batch_normalization_249[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_254 (ReLU)                (None, 13, 13, 28)   0           batch_normalization_254[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_256 (Conv2D)             (None, 13, 13, 16)   2048        average_pooling2d_25[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_257 (Conv2D)             (None, 13, 13, 48)   6144        re_lu_246[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_260 (Conv2D)             (None, 13, 13, 32)   6272        re_lu_249[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_265 (Conv2D)             (None, 13, 13, 32)   6272        re_lu_254[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 13, 13, 128)  0           conv2d_256[0][0]                 \n",
            "                                                                 conv2d_257[0][0]                 \n",
            "                                                                 conv2d_260[0][0]                 \n",
            "                                                                 conv2d_265[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_255 (BatchN (None, 13, 13, 128)  512         concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_255 (ReLU)                (None, 13, 13, 128)  0           batch_normalization_255[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_266 (Conv2D)             (None, 13, 13, 24)   3072        re_lu_255[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_256 (BatchN (None, 13, 13, 24)   96          conv2d_266[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_260 (BatchN (None, 13, 13, 128)  512         concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_256 (ReLU)                (None, 13, 13, 24)   0           batch_normalization_256[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_260 (ReLU)                (None, 13, 13, 128)  0           batch_normalization_260[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 6, 6, 128)    0           concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_267 (Conv2D)             (None, 6, 6, 24)     5184        re_lu_256[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_271 (Conv2D)             (None, 6, 6, 40)     46080       re_lu_260[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 6, 6, 192)    0           max_pooling2d_5[0][0]            \n",
            "                                                                 conv2d_267[0][0]                 \n",
            "                                                                 conv2d_271[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_265 (BatchN (None, 6, 6, 192)    768         concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_265 (ReLU)                (None, 6, 6, 192)    0           batch_normalization_265[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_277 (Conv2D)             (None, 6, 6, 48)     9216        re_lu_265[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_266 (BatchN (None, 6, 6, 48)     192         conv2d_277[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_266 (ReLU)                (None, 6, 6, 48)     0           batch_normalization_266[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_278 (Conv2D)             (None, 6, 6, 56)     8064        re_lu_266[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_263 (BatchN (None, 6, 6, 192)    768         concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_267 (BatchN (None, 6, 6, 56)     224         conv2d_278[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_263 (ReLU)                (None, 6, 6, 192)    0           batch_normalization_263[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_267 (ReLU)                (None, 6, 6, 56)     0           batch_normalization_267[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_261 (BatchN (None, 6, 6, 192)    768         concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_274 (Conv2D)             (None, 6, 6, 48)     9216        re_lu_263[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_279 (Conv2D)             (None, 6, 6, 64)     10752       re_lu_267[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_261 (ReLU)                (None, 6, 6, 192)    0           batch_normalization_261[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_262 (BatchN (None, 6, 6, 192)    768         concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_264 (BatchN (None, 6, 6, 48)     192         conv2d_274[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_268 (BatchN (None, 6, 6, 64)     256         conv2d_279[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_26 (AveragePo (None, 6, 6, 192)    0           re_lu_261[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_262 (ReLU)                (None, 6, 6, 192)    0           batch_normalization_262[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_264 (ReLU)                (None, 6, 6, 48)     0           batch_normalization_264[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_268 (ReLU)                (None, 6, 6, 64)     0           batch_normalization_268[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_272 (Conv2D)             (None, 6, 6, 32)     6144        average_pooling2d_26[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_273 (Conv2D)             (None, 6, 6, 32)     6144        re_lu_262[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_275 (Conv2D)             (None, 6, 6, 32)     4608        re_lu_264[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_276 (Conv2D)             (None, 6, 6, 32)     4608        re_lu_264[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_280 (Conv2D)             (None, 6, 6, 32)     6144        re_lu_268[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_281 (Conv2D)             (None, 6, 6, 32)     6144        re_lu_268[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 6, 6, 192)    0           conv2d_272[0][0]                 \n",
            "                                                                 conv2d_273[0][0]                 \n",
            "                                                                 conv2d_275[0][0]                 \n",
            "                                                                 conv2d_276[0][0]                 \n",
            "                                                                 conv2d_280[0][0]                 \n",
            "                                                                 conv2d_281[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_269 (BatchN (None, 6, 6, 192)    768         concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_269 (ReLU)                (None, 6, 6, 192)    0           batch_normalization_269[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_2 (Glo (None, 192)          0           re_lu_269[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 192)          0           global_average_pooling2d_2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           1930        dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 268,502\n",
            "Trainable params: 262,638\n",
            "Non-trainable params: 5,864\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUIYhSXbm5Wt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.utils.plot_model(model, 'cifar_model.png', show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-DCzFIrsLjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learning rate schedule\n",
        "def lr_schedule(epoch):\n",
        "\n",
        "    # staircase\n",
        "    # lr = TRAINING_LR_MAX*math.pow(TRAINING_LR_SCALE, math.floor(epoch/TRAINING_LR_EPOCHS))\n",
        "\n",
        "    # linear warmup followed by cosine decay\n",
        "    if epoch < TRAINING_LR_INIT_EPOCHS:\n",
        "        lr = (TRAINING_LR_MAX - TRAINING_LR_INIT)*(float(epoch)/TRAINING_LR_INIT_EPOCHS) + TRAINING_LR_INIT\n",
        "    else:\n",
        "        lr = (TRAINING_LR_MAX - TRAINING_LR_FINAL)*max(0.0, math.cos(((float(epoch) - TRAINING_LR_INIT_EPOCHS)/(TRAINING_LR_FINAL_EPOCHS - 1.0))*(math.pi/2.0))) + TRAINING_LR_FINAL\n",
        "\n",
        "    # debug - learning rate display\n",
        "    # print(epoch)\n",
        "    # print(lr)\n",
        "\n",
        "    return lr\n",
        "\n",
        "# plot training accuracy and loss curves\n",
        "def plot_training_curves(history):\n",
        "\n",
        "    # training and validation data accuracy\n",
        "    acc     = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "\n",
        "    # training and validation data loss\n",
        "    loss     = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    # plot accuracy\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(acc, label='Training Accuracy')\n",
        "    plt.plot(val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim([min(plt.ylim()), 1])\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    # plot loss\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(loss, label='Training Loss')\n",
        "    plt.plot(val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.ylabel('Cross Entropy')\n",
        "    plt.ylim([0, 2.0])\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfG75Ggisqox",
        "colab_type": "code",
        "outputId": "9ddb8068-76f8-4a33-ce74-0cc4d699987b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# callbacks (learning rate schedule, model checkpointing during training)\n",
        "callbacks = [keras.callbacks.LearningRateScheduler(lr_schedule),\n",
        "             keras.callbacks.ModelCheckpoint(filepath=SAVE_MODEL_PATH+'model_{epoch}.h5', save_best_only=True, monitor='val_loss', verbose=1)]\n",
        "\n",
        "# training\n",
        "initial_epoch_num = 0\n",
        "history           = model.fit(x=dataset_train, epochs=TRAINING_NUM_EPOCHS, verbose=1, callbacks=callbacks, validation_data=dataset_test, initial_epoch=initial_epoch_num)\n",
        "\n",
        "# example of restarting training after a crash from the last saved checkpoint\n",
        "# model             = create_model(MODEL_LEVEL_0_REPEATS, MODEL_LEVEL_1_REPEATS, MODEL_LEVEL_2_REPEATS)\n",
        "# model.load_weights(SAVE_MODEL_PATH+'model_X.h5') # replace X with the last saved checkpoint number\n",
        "# initial_epoch_num = X                            # replace X with the last saved checkpoint number\n",
        "# history           = model.fit(x=dataset_train, epochs=TRAINING_NUM_EPOCHS, verbose=1, callbacks=callbacks, validation_data=dataset_test, initial_epoch=initial_epoch_num)\n",
        "\n",
        "# plot accuracy and loss curves\n",
        "plot_training_curves(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   1563/Unknown - 201s 128ms/step - loss: 2.3175 - accuracy: 0.1474\n",
            "Epoch 00001: val_loss improved from inf to 2.10776, saving model to /content/drive/My Drive/save/model/model_1.h5\n",
            "1563/1563 [==============================] - 211s 135ms/step - loss: 2.3175 - accuracy: 0.1474 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.5715 - accuracy: 0.4217\n",
            "Epoch 00002: val_loss improved from 2.10776 to 1.33636, saving model to /content/drive/My Drive/save/model/model_2.h5\n",
            "1563/1563 [==============================] - 178s 114ms/step - loss: 1.5714 - accuracy: 0.4217 - val_loss: 1.3364 - val_accuracy: 0.5170\n",
            "Epoch 3/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.2042 - accuracy: 0.5693\n",
            "Epoch 00003: val_loss improved from 1.33636 to 1.06584, saving model to /content/drive/My Drive/save/model/model_3.h5\n",
            "1563/1563 [==============================] - 177s 113ms/step - loss: 1.2041 - accuracy: 0.5693 - val_loss: 1.0658 - val_accuracy: 0.6198\n",
            "Epoch 4/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0313 - accuracy: 0.6368\n",
            "Epoch 00004: val_loss improved from 1.06584 to 0.98253, saving model to /content/drive/My Drive/save/model/model_4.h5\n",
            "1563/1563 [==============================] - 177s 113ms/step - loss: 1.0315 - accuracy: 0.6367 - val_loss: 0.9825 - val_accuracy: 0.6521\n",
            "Epoch 5/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.9047 - accuracy: 0.6828\n",
            "Epoch 00005: val_loss improved from 0.98253 to 0.84768, saving model to /content/drive/My Drive/save/model/model_5.h5\n",
            "1563/1563 [==============================] - 176s 113ms/step - loss: 0.9046 - accuracy: 0.6828 - val_loss: 0.8477 - val_accuracy: 0.7071\n",
            "Epoch 6/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.8199 - accuracy: 0.7142\n",
            "Epoch 00006: val_loss did not improve from 0.84768\n",
            "1563/1563 [==============================] - 176s 112ms/step - loss: 0.8196 - accuracy: 0.7142 - val_loss: 0.9727 - val_accuracy: 0.6564\n",
            "Epoch 7/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.7233 - accuracy: 0.7490\n",
            "Epoch 00007: val_loss improved from 0.84768 to 0.73735, saving model to /content/drive/My Drive/save/model/model_7.h5\n",
            "1563/1563 [==============================] - 176s 113ms/step - loss: 0.7231 - accuracy: 0.7491 - val_loss: 0.7374 - val_accuracy: 0.7461\n",
            "Epoch 8/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6683 - accuracy: 0.7697\n",
            "Epoch 00008: val_loss did not improve from 0.73735\n",
            "1563/1563 [==============================] - 175s 112ms/step - loss: 0.6681 - accuracy: 0.7697 - val_loss: 0.7682 - val_accuracy: 0.7479\n",
            "Epoch 9/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6206 - accuracy: 0.7852\n",
            "Epoch 00009: val_loss improved from 0.73735 to 0.62100, saving model to /content/drive/My Drive/save/model/model_9.h5\n",
            "1563/1563 [==============================] - 177s 113ms/step - loss: 0.6205 - accuracy: 0.7852 - val_loss: 0.6210 - val_accuracy: 0.7886\n",
            "Epoch 10/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5730 - accuracy: 0.8018\n",
            "Epoch 00010: val_loss improved from 0.62100 to 0.57528, saving model to /content/drive/My Drive/save/model/model_10.h5\n",
            "1563/1563 [==============================] - 176s 112ms/step - loss: 0.5732 - accuracy: 0.8017 - val_loss: 0.5753 - val_accuracy: 0.8012\n",
            "Epoch 11/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5456 - accuracy: 0.8116\n",
            "Epoch 00011: val_loss improved from 0.57528 to 0.54917, saving model to /content/drive/My Drive/save/model/model_11.h5\n",
            "1563/1563 [==============================] - 177s 113ms/step - loss: 0.5454 - accuracy: 0.8117 - val_loss: 0.5492 - val_accuracy: 0.8128\n",
            "Epoch 12/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5143 - accuracy: 0.8215\n",
            "Epoch 00012: val_loss improved from 0.54917 to 0.51232, saving model to /content/drive/My Drive/save/model/model_12.h5\n",
            "1563/1563 [==============================] - 177s 113ms/step - loss: 0.5144 - accuracy: 0.8215 - val_loss: 0.5123 - val_accuracy: 0.8248\n",
            "Epoch 13/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.4884 - accuracy: 0.8322\n",
            "Epoch 00013: val_loss did not improve from 0.51232\n",
            "1563/1563 [==============================] - 177s 113ms/step - loss: 0.4883 - accuracy: 0.8322 - val_loss: 0.5365 - val_accuracy: 0.8175\n",
            "Epoch 14/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.4685 - accuracy: 0.8384\n",
            "Epoch 00014: val_loss improved from 0.51232 to 0.50909, saving model to /content/drive/My Drive/save/model/model_14.h5\n",
            "1563/1563 [==============================] - 178s 114ms/step - loss: 0.4684 - accuracy: 0.8384 - val_loss: 0.5091 - val_accuracy: 0.8258\n",
            "Epoch 15/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.4460 - accuracy: 0.8452\n",
            "Epoch 00015: val_loss improved from 0.50909 to 0.50225, saving model to /content/drive/My Drive/save/model/model_15.h5\n",
            "1563/1563 [==============================] - 177s 113ms/step - loss: 0.4465 - accuracy: 0.8451 - val_loss: 0.5023 - val_accuracy: 0.8290\n",
            "Epoch 16/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.4279 - accuracy: 0.8540\n",
            "Epoch 00016: val_loss improved from 0.50225 to 0.49917, saving model to /content/drive/My Drive/save/model/model_16.h5\n",
            "1563/1563 [==============================] - 177s 114ms/step - loss: 0.4282 - accuracy: 0.8539 - val_loss: 0.4992 - val_accuracy: 0.8309\n",
            "Epoch 17/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.4077 - accuracy: 0.8598\n",
            "Epoch 00017: val_loss improved from 0.49917 to 0.48752, saving model to /content/drive/My Drive/save/model/model_17.h5\n",
            "1563/1563 [==============================] - 176s 112ms/step - loss: 0.4077 - accuracy: 0.8598 - val_loss: 0.4875 - val_accuracy: 0.8350\n",
            "Epoch 18/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.3956 - accuracy: 0.8633\n",
            "Epoch 00018: val_loss did not improve from 0.48752\n",
            "1563/1563 [==============================] - 176s 112ms/step - loss: 0.3957 - accuracy: 0.8633 - val_loss: 0.5085 - val_accuracy: 0.8261\n",
            "Epoch 19/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.3820 - accuracy: 0.8686\n",
            "Epoch 00019: val_loss improved from 0.48752 to 0.43740, saving model to /content/drive/My Drive/save/model/model_19.h5\n",
            "1563/1563 [==============================] - 175s 112ms/step - loss: 0.3823 - accuracy: 0.8686 - val_loss: 0.4374 - val_accuracy: 0.8501\n",
            "Epoch 20/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.3735 - accuracy: 0.8708\n",
            "Epoch 00020: val_loss did not improve from 0.43740\n",
            "1563/1563 [==============================] - 175s 112ms/step - loss: 0.3737 - accuracy: 0.8708 - val_loss: 0.4395 - val_accuracy: 0.8463\n",
            "Epoch 21/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.3538 - accuracy: 0.8767\n",
            "Epoch 00021: val_loss improved from 0.43740 to 0.43107, saving model to /content/drive/My Drive/save/model/model_21.h5\n",
            "1563/1563 [==============================] - 177s 113ms/step - loss: 0.3539 - accuracy: 0.8767 - val_loss: 0.4311 - val_accuracy: 0.8564\n",
            "Epoch 22/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.3492 - accuracy: 0.8808\n",
            "Epoch 00022: val_loss improved from 0.43107 to 0.42119, saving model to /content/drive/My Drive/save/model/model_22.h5\n",
            "1563/1563 [==============================] - 176s 113ms/step - loss: 0.3491 - accuracy: 0.8808 - val_loss: 0.4212 - val_accuracy: 0.8580\n",
            "Epoch 23/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.3317 - accuracy: 0.8839\n",
            "Epoch 00023: val_loss did not improve from 0.42119\n",
            "1563/1563 [==============================] - 176s 113ms/step - loss: 0.3318 - accuracy: 0.8839 - val_loss: 0.4558 - val_accuracy: 0.8457\n",
            "Epoch 24/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.3236 - accuracy: 0.8885\n",
            "Epoch 00024: val_loss improved from 0.42119 to 0.41929, saving model to /content/drive/My Drive/save/model/model_24.h5\n",
            "1563/1563 [==============================] - 176s 112ms/step - loss: 0.3235 - accuracy: 0.8886 - val_loss: 0.4193 - val_accuracy: 0.8559\n",
            "Epoch 25/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.3151 - accuracy: 0.8907\n",
            "Epoch 00025: val_loss improved from 0.41929 to 0.40240, saving model to /content/drive/My Drive/save/model/model_25.h5\n",
            "1563/1563 [==============================] - 176s 112ms/step - loss: 0.3151 - accuracy: 0.8907 - val_loss: 0.4024 - val_accuracy: 0.8652\n",
            "Epoch 26/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.3021 - accuracy: 0.8945\n",
            "Epoch 00026: val_loss improved from 0.40240 to 0.39526, saving model to /content/drive/My Drive/save/model/model_26.h5\n",
            "1563/1563 [==============================] - 175s 112ms/step - loss: 0.3021 - accuracy: 0.8945 - val_loss: 0.3953 - val_accuracy: 0.8663\n",
            "Epoch 27/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2991 - accuracy: 0.8964\n",
            "Epoch 00027: val_loss did not improve from 0.39526\n",
            "1563/1563 [==============================] - 176s 113ms/step - loss: 0.2990 - accuracy: 0.8965 - val_loss: 0.4032 - val_accuracy: 0.8652\n",
            "Epoch 28/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2859 - accuracy: 0.9006\n",
            "Epoch 00028: val_loss did not improve from 0.39526\n",
            "1563/1563 [==============================] - 177s 113ms/step - loss: 0.2860 - accuracy: 0.9006 - val_loss: 0.4190 - val_accuracy: 0.8629\n",
            "Epoch 29/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2816 - accuracy: 0.9011\n",
            "Epoch 00029: val_loss did not improve from 0.39526\n",
            "1563/1563 [==============================] - 177s 113ms/step - loss: 0.2816 - accuracy: 0.9011 - val_loss: 0.3976 - val_accuracy: 0.8648\n",
            "Epoch 30/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2711 - accuracy: 0.9064\n",
            "Epoch 00030: val_loss improved from 0.39526 to 0.39449, saving model to /content/drive/My Drive/save/model/model_30.h5\n",
            "1563/1563 [==============================] - 178s 114ms/step - loss: 0.2711 - accuracy: 0.9063 - val_loss: 0.3945 - val_accuracy: 0.8695\n",
            "Epoch 31/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2613 - accuracy: 0.9087\n",
            "Epoch 00031: val_loss did not improve from 0.39449\n",
            "1563/1563 [==============================] - 178s 114ms/step - loss: 0.2612 - accuracy: 0.9087 - val_loss: 0.4009 - val_accuracy: 0.8671\n",
            "Epoch 32/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2526 - accuracy: 0.9137\n",
            "Epoch 00032: val_loss improved from 0.39449 to 0.39131, saving model to /content/drive/My Drive/save/model/model_32.h5\n",
            "1563/1563 [==============================] - 178s 114ms/step - loss: 0.2525 - accuracy: 0.9137 - val_loss: 0.3913 - val_accuracy: 0.8714\n",
            "Epoch 33/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2517 - accuracy: 0.9129\n",
            "Epoch 00033: val_loss improved from 0.39131 to 0.39008, saving model to /content/drive/My Drive/save/model/model_33.h5\n",
            "1563/1563 [==============================] - 177s 113ms/step - loss: 0.2518 - accuracy: 0.9129 - val_loss: 0.3901 - val_accuracy: 0.8710\n",
            "Epoch 34/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2393 - accuracy: 0.9166\n",
            "Epoch 00034: val_loss improved from 0.39008 to 0.36765, saving model to /content/drive/My Drive/save/model/model_34.h5\n",
            "1563/1563 [==============================] - 178s 114ms/step - loss: 0.2392 - accuracy: 0.9165 - val_loss: 0.3676 - val_accuracy: 0.8773\n",
            "Epoch 35/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2351 - accuracy: 0.9181\n",
            "Epoch 00035: val_loss improved from 0.36765 to 0.36280, saving model to /content/drive/My Drive/save/model/model_35.h5\n",
            "1563/1563 [==============================] - 178s 114ms/step - loss: 0.2350 - accuracy: 0.9181 - val_loss: 0.3628 - val_accuracy: 0.8802\n",
            "Epoch 36/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2297 - accuracy: 0.9184\n",
            "Epoch 00036: val_loss did not improve from 0.36280\n",
            "1563/1563 [==============================] - 177s 113ms/step - loss: 0.2298 - accuracy: 0.9184 - val_loss: 0.3786 - val_accuracy: 0.8819\n",
            "Epoch 37/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2212 - accuracy: 0.9233\n",
            "Epoch 00037: val_loss did not improve from 0.36280\n",
            "1563/1563 [==============================] - 177s 113ms/step - loss: 0.2211 - accuracy: 0.9233 - val_loss: 0.3886 - val_accuracy: 0.8756\n",
            "Epoch 38/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2153 - accuracy: 0.9245\n",
            "Epoch 00038: val_loss did not improve from 0.36280\n",
            "1563/1563 [==============================] - 176s 113ms/step - loss: 0.2152 - accuracy: 0.9245 - val_loss: 0.3717 - val_accuracy: 0.8798\n",
            "Epoch 39/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2068 - accuracy: 0.9258\n",
            "Epoch 00039: val_loss improved from 0.36280 to 0.35342, saving model to /content/drive/My Drive/save/model/model_39.h5\n",
            "1563/1563 [==============================] - 176s 113ms/step - loss: 0.2067 - accuracy: 0.9258 - val_loss: 0.3534 - val_accuracy: 0.8840\n",
            "Epoch 40/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2043 - accuracy: 0.9286\n",
            "Epoch 00040: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 175s 112ms/step - loss: 0.2045 - accuracy: 0.9286 - val_loss: 0.3650 - val_accuracy: 0.8817\n",
            "Epoch 41/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1950 - accuracy: 0.9306\n",
            "Epoch 00041: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 175s 112ms/step - loss: 0.1953 - accuracy: 0.9306 - val_loss: 0.3868 - val_accuracy: 0.8805\n",
            "Epoch 42/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1901 - accuracy: 0.9327\n",
            "Epoch 00042: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 176s 112ms/step - loss: 0.1901 - accuracy: 0.9327 - val_loss: 0.3876 - val_accuracy: 0.8779\n",
            "Epoch 43/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1839 - accuracy: 0.9348\n",
            "Epoch 00043: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 175s 112ms/step - loss: 0.1839 - accuracy: 0.9348 - val_loss: 0.3806 - val_accuracy: 0.8815\n",
            "Epoch 44/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1787 - accuracy: 0.9366\n",
            "Epoch 00044: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 176s 113ms/step - loss: 0.1786 - accuracy: 0.9366 - val_loss: 0.3825 - val_accuracy: 0.8835\n",
            "Epoch 45/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1717 - accuracy: 0.9396\n",
            "Epoch 00045: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 176s 113ms/step - loss: 0.1717 - accuracy: 0.9396 - val_loss: 0.3769 - val_accuracy: 0.8890\n",
            "Epoch 46/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1668 - accuracy: 0.9408\n",
            "Epoch 00046: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 176s 113ms/step - loss: 0.1668 - accuracy: 0.9407 - val_loss: 0.3765 - val_accuracy: 0.8840\n",
            "Epoch 47/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1609 - accuracy: 0.9430\n",
            "Epoch 00047: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 176s 113ms/step - loss: 0.1608 - accuracy: 0.9430 - val_loss: 0.3713 - val_accuracy: 0.8882\n",
            "Epoch 48/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1551 - accuracy: 0.9458\n",
            "Epoch 00048: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 176s 112ms/step - loss: 0.1550 - accuracy: 0.9458 - val_loss: 0.3701 - val_accuracy: 0.8871\n",
            "Epoch 49/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1522 - accuracy: 0.9471\n",
            "Epoch 00049: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 177s 113ms/step - loss: 0.1523 - accuracy: 0.9470 - val_loss: 0.3635 - val_accuracy: 0.8899\n",
            "Epoch 50/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1419 - accuracy: 0.9498\n",
            "Epoch 00050: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 175s 112ms/step - loss: 0.1419 - accuracy: 0.9498 - val_loss: 0.3716 - val_accuracy: 0.8894\n",
            "Epoch 51/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1391 - accuracy: 0.9507\n",
            "Epoch 00051: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 176s 113ms/step - loss: 0.1391 - accuracy: 0.9507 - val_loss: 0.3699 - val_accuracy: 0.8908\n",
            "Epoch 52/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.9542\n",
            "Epoch 00052: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 175s 112ms/step - loss: 0.1303 - accuracy: 0.9542 - val_loss: 0.3750 - val_accuracy: 0.8863\n",
            "Epoch 53/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1282 - accuracy: 0.9549\n",
            "Epoch 00053: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 176s 113ms/step - loss: 0.1281 - accuracy: 0.9549 - val_loss: 0.3713 - val_accuracy: 0.8929\n",
            "Epoch 54/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9555\n",
            "Epoch 00054: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 174s 112ms/step - loss: 0.1254 - accuracy: 0.9555 - val_loss: 0.3714 - val_accuracy: 0.8899\n",
            "Epoch 55/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1180 - accuracy: 0.9594\n",
            "Epoch 00055: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 174s 112ms/step - loss: 0.1182 - accuracy: 0.9594 - val_loss: 0.3779 - val_accuracy: 0.8885\n",
            "Epoch 56/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1138 - accuracy: 0.9600\n",
            "Epoch 00056: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 174s 112ms/step - loss: 0.1138 - accuracy: 0.9600 - val_loss: 0.3733 - val_accuracy: 0.8917\n",
            "Epoch 57/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1127 - accuracy: 0.9593\n",
            "Epoch 00057: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 173s 110ms/step - loss: 0.1127 - accuracy: 0.9593 - val_loss: 0.3745 - val_accuracy: 0.8914\n",
            "Epoch 58/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1059 - accuracy: 0.9630\n",
            "Epoch 00058: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 175s 112ms/step - loss: 0.1059 - accuracy: 0.9630 - val_loss: 0.3722 - val_accuracy: 0.8927\n",
            "Epoch 59/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1077 - accuracy: 0.9618\n",
            "Epoch 00059: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 178s 114ms/step - loss: 0.1077 - accuracy: 0.9618 - val_loss: 0.3710 - val_accuracy: 0.8920\n",
            "Epoch 60/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1022 - accuracy: 0.9638\n",
            "Epoch 00060: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 178s 114ms/step - loss: 0.1022 - accuracy: 0.9638 - val_loss: 0.3705 - val_accuracy: 0.8928\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAHwCAYAAAChTMYRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nF0NCAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMLtCygwsurA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = create_model(28, 28, 3, DATA_NUM_CLASSES, TRAINING_LR_MAX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOCxjbQ2tKfB",
        "colab_type": "code",
        "outputId": "48aac52c-9675-4654-e856-9c4522cb5bd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "# training\n",
        "initial_epoch_num = 0\n",
        "history2           = model2.fit(x=dataset_train, epochs=TRAINING_NUM_EPOCHS, verbose=1, callbacks=callbacks, validation_data=dataset_test, initial_epoch=initial_epoch_num)\n",
        "\n",
        "# example of restarting training after a crash from the last saved checkpoint\n",
        "# model             = create_model(MODEL_LEVEL_0_REPEATS, MODEL_LEVEL_1_REPEATS, MODEL_LEVEL_2_REPEATS)\n",
        "# model.load_weights(SAVE_MODEL_PATH+'model_X.h5') # replace X with the last saved checkpoint number\n",
        "# initial_epoch_num = X                            # replace X with the last saved checkpoint number\n",
        "# history           = model.fit(x=dataset_train, epochs=TRAINING_NUM_EPOCHS, verbose=1, callbacks=callbacks, validation_data=dataset_test, initial_epoch=initial_epoch_num)\n",
        "\n",
        "# plot accuracy and loss curves\n",
        "plot_training_curves(history2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "   1563/Unknown - 774s 495ms/step - loss: 2.3368 - accuracy: 0.1169\n",
            "Epoch 00001: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 802s 513ms/step - loss: 2.3368 - accuracy: 0.1169 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 2.1298 - accuracy: 0.2037\n",
            "Epoch 00002: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 510s 326ms/step - loss: 2.1296 - accuracy: 0.2037 - val_loss: 1.9303 - val_accuracy: 0.2816\n",
            "Epoch 3/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.8209 - accuracy: 0.3197\n",
            "Epoch 00003: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 518s 331ms/step - loss: 1.8207 - accuracy: 0.3197 - val_loss: 1.6319 - val_accuracy: 0.4047\n",
            "Epoch 4/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.5904 - accuracy: 0.4169\n",
            "Epoch 00004: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 514s 329ms/step - loss: 1.5902 - accuracy: 0.4170 - val_loss: 1.6200 - val_accuracy: 0.4048\n",
            "Epoch 5/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.3976 - accuracy: 0.4975\n",
            "Epoch 00005: val_loss did not improve from 0.35342\n",
            "1563/1563 [==============================] - 520s 332ms/step - loss: 1.3975 - accuracy: 0.4975 - val_loss: 1.3656 - val_accuracy: 0.5232\n",
            "Epoch 6/60\n",
            " 253/1563 [===>..........................] - ETA: 7:05 - loss: 1.3387 - accuracy: 0.5217"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-764e7fbd1d6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minitial_epoch_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory2\u001b[0m           \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAINING_NUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# example of restarting training after a crash from the last saved checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# model             = create_model(MODEL_LEVEL_0_REPEATS, MODEL_LEVEL_1_REPEATS, MODEL_LEVEL_2_REPEATS)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3510\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSg3W_M-HvD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}